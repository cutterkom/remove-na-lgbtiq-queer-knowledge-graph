{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-14 12:00:17.300 | WARNING  | rubrix.client.rubrix_client:load:310 - The argument 'as_pandas' in `rb.load` will be deprecated in the future, and we will always return a `Dataset`. To emulate the future behavior set `as_pandas=False`. To get a pandas DataFrame, call `Dataset.to_pandas()`\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>prediction</th>\n",
       "      <th>prediction_agent</th>\n",
       "      <th>annotation</th>\n",
       "      <th>annotation_agent</th>\n",
       "      <th>id</th>\n",
       "      <th>metadata</th>\n",
       "      <th>status</th>\n",
       "      <th>event_timestamp</th>\n",
       "      <th>metrics</th>\n",
       "      <th>search_keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Teestube - In der Pestalozzistraße 20 Rgb. erö...</td>\n",
       "      <td>[Teestube, -, In, der, Pestalozzistraße, 20, R...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[(LOC, 0, 8), (ADR, 18, 37), (ADR, 104, 121), ...</td>\n",
       "      <td>rubrix</td>\n",
       "      <td>00f2ae40-e155-4f98-adaa-61d1db5a08ff</td>\n",
       "      <td>{'date': '22. Juni 1974', 'year': 1974, 'id': ...</td>\n",
       "      <td>Validated</td>\n",
       "      <td>None</td>\n",
       "      <td>{'text_length': 350, 'tokens': [{'idx': 0, 'va...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Adelheid Lissmann - Adelheid Lissmann, geb. 19...</td>\n",
       "      <td>[Adelheid, Lissmann, -, Adelheid, Lissmann, ,,...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[(PER, 0, 17), (ORG, 74, 95)]</td>\n",
       "      <td>rubrix</td>\n",
       "      <td>01a4910f-1088-413a-8879-55f06ded5d20</td>\n",
       "      <td>{'date': '1946', 'year': 1946, 'id': 12}</td>\n",
       "      <td>Validated</td>\n",
       "      <td>None</td>\n",
       "      <td>{'text_length': 192, 'tokens': [{'idx': 0, 'va...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Beschlagnahmen - Beschlagnahme der Blätter für...</td>\n",
       "      <td>[Beschlagnahmen, -, Beschlagnahme, der, Blätte...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[(PUBLICATION, 35, 60), (PUBLICATION, 71, 90)]</td>\n",
       "      <td>rubrix</td>\n",
       "      <td>029c88d4-bda1-4e3c-af72-85e516588301</td>\n",
       "      <td>{'date': '1925', 'year': 1925, 'id': 28}</td>\n",
       "      <td>Validated</td>\n",
       "      <td>None</td>\n",
       "      <td>{'text_length': 266, 'tokens': [{'idx': 0, 'va...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lesbenfrühlings­treffen - Das Lesbenfrühlingst...</td>\n",
       "      <td>[Lesbenfrühlings­treffen, -, Das, Lesbenfrühli...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[(EVENT, 0, 23), (EVENT, 30, 52), (EVENT, 61, ...</td>\n",
       "      <td>rubrix</td>\n",
       "      <td>02c3041b-6122-4f61-aa1e-26001b2cc865</td>\n",
       "      <td>{'date': '24. – 27. Mai 1996', 'year': 1996, '...</td>\n",
       "      <td>Validated</td>\n",
       "      <td>None</td>\n",
       "      <td>{'text_length': 210, 'tokens': [{'idx': 0, 'va...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Moby Dyke - Die Kunstaktion „Moby Dyke Lesbian...</td>\n",
       "      <td>[Moby, Dyke, -, Die, Kunstaktion, „, Moby, Dyk...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[(PER, 56, 70), (PER, 75, 93), (LOC, 118, 138)...</td>\n",
       "      <td>rubrix</td>\n",
       "      <td>03bf56b1-0857-47fc-9588-5caf5929847e</td>\n",
       "      <td>{'date': '21. – 22. August 2015', 'year': 2015...</td>\n",
       "      <td>Validated</td>\n",
       "      <td>None</td>\n",
       "      <td>{'text_length': 193, 'tokens': [{'idx': 0, 'va...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  Teestube - In der Pestalozzistraße 20 Rgb. erö...   \n",
       "1  Adelheid Lissmann - Adelheid Lissmann, geb. 19...   \n",
       "2  Beschlagnahmen - Beschlagnahme der Blätter für...   \n",
       "3  Lesbenfrühlings­treffen - Das Lesbenfrühlingst...   \n",
       "4  Moby Dyke - Die Kunstaktion „Moby Dyke Lesbian...   \n",
       "\n",
       "                                              tokens prediction  \\\n",
       "0  [Teestube, -, In, der, Pestalozzistraße, 20, R...       None   \n",
       "1  [Adelheid, Lissmann, -, Adelheid, Lissmann, ,,...       None   \n",
       "2  [Beschlagnahmen, -, Beschlagnahme, der, Blätte...       None   \n",
       "3  [Lesbenfrühlings­treffen, -, Das, Lesbenfrühli...       None   \n",
       "4  [Moby, Dyke, -, Die, Kunstaktion, „, Moby, Dyk...       None   \n",
       "\n",
       "  prediction_agent                                         annotation  \\\n",
       "0             None  [(LOC, 0, 8), (ADR, 18, 37), (ADR, 104, 121), ...   \n",
       "1             None                      [(PER, 0, 17), (ORG, 74, 95)]   \n",
       "2             None     [(PUBLICATION, 35, 60), (PUBLICATION, 71, 90)]   \n",
       "3             None  [(EVENT, 0, 23), (EVENT, 30, 52), (EVENT, 61, ...   \n",
       "4             None  [(PER, 56, 70), (PER, 75, 93), (LOC, 118, 138)...   \n",
       "\n",
       "  annotation_agent                                    id  \\\n",
       "0           rubrix  00f2ae40-e155-4f98-adaa-61d1db5a08ff   \n",
       "1           rubrix  01a4910f-1088-413a-8879-55f06ded5d20   \n",
       "2           rubrix  029c88d4-bda1-4e3c-af72-85e516588301   \n",
       "3           rubrix  02c3041b-6122-4f61-aa1e-26001b2cc865   \n",
       "4           rubrix  03bf56b1-0857-47fc-9588-5caf5929847e   \n",
       "\n",
       "                                            metadata     status  \\\n",
       "0  {'date': '22. Juni 1974', 'year': 1974, 'id': ...  Validated   \n",
       "1           {'date': '1946', 'year': 1946, 'id': 12}  Validated   \n",
       "2           {'date': '1925', 'year': 1925, 'id': 28}  Validated   \n",
       "3  {'date': '24. – 27. Mai 1996', 'year': 1996, '...  Validated   \n",
       "4  {'date': '21. – 22. August 2015', 'year': 2015...  Validated   \n",
       "\n",
       "  event_timestamp                                            metrics  \\\n",
       "0            None  {'text_length': 350, 'tokens': [{'idx': 0, 'va...   \n",
       "1            None  {'text_length': 192, 'tokens': [{'idx': 0, 'va...   \n",
       "2            None  {'text_length': 266, 'tokens': [{'idx': 0, 'va...   \n",
       "3            None  {'text_length': 210, 'tokens': [{'idx': 0, 'va...   \n",
       "4            None  {'text_length': 193, 'tokens': [{'idx': 0, 'va...   \n",
       "\n",
       "  search_keywords  \n",
       "0            None  \n",
       "1            None  \n",
       "2            None  \n",
       "3            None  \n",
       "4            None  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "import rubrix as rb\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# load rubrix dataset\n",
    "# select text input and the annotated label\n",
    "# https://github.com/recognai/rubrix#3-load-and-create-a-training-set\n",
    "dataset_rb = rb.load('chronik_annotations', query=\"status:Validated\")\n",
    "dataset_rb.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "The data needs two steps of preprocessing:\n",
    "\n",
    "1. Convert `rubrix` to `spacy` format for annotations, because `rubrix` exports the annotated information as `(label, start, end)`, but `spacy` needs `(start, end, label)`. See: `convert_rubrix_to_spacy()`.\n",
    "2. Convert annotations to a `DocBin` and save file to disk. See `create_doc_bin()`\n",
    "\n",
    "\n",
    "[More infos in official Documentation](https://spacy.io/usage/training#training-data):\n",
    "\n",
    "> For example, if you’re creating an NER pipeline, loading your annotations and setting them as the .ents property on a Doc is all you need to worry about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_rubrix_to_spacy(rubrix_name:str, query:str, filter_labels:bool, translate_labels:bool):\n",
    "    \"\"\"import annotated data from rubrix and transform it to spacy flavour\n",
    "\n",
    "    Args:\n",
    "        rubrix_name (str): name of dataset in rubrix webapp\n",
    "        query (str): query of rubrix data, typically `\"status:Validated\"` if manually labeled data\n",
    "        filter_labels (bool): use all labels or subpart. If subpart, then provide a list called keep_label = [\"PER\", \"LOC\", \"EVENT\"]\n",
    "        translate_labels (bool): use if some labels should be translated, in order to get more items per label. If so, provide a dict called label_dict = {\"CLUB\":\"ORG\", \"PARTY\":\"EVENT\", \"MOVEMENT\":\"EVENT\", \"AWARD\": \"EVENT\"}\n",
    "    \"\"\"\n",
    "    # load rubrix dataset\n",
    "    labeled_data = rb.load(rubrix_name, query=query)\n",
    "\n",
    "    labeled_data_df = pd.DataFrame({\n",
    "        \"text\": labeled_data.text,\n",
    "        \"label\": labeled_data.annotation,\n",
    "    })\n",
    "    \n",
    "    training_data = []\n",
    "\n",
    "    for record in labeled_data_df.index:\n",
    "        entities = []\n",
    "        text = labeled_data_df[\"text\"][record]\n",
    "        labels = labeled_data_df[\"label\"][record]\n",
    "        \n",
    "        for label in labels:\n",
    "            \n",
    "            start = label[1]\n",
    "            end = label[2]\n",
    "            label = label[0]\n",
    "            # switch position:\n",
    "            entity = text, label, start, end\n",
    "\n",
    "            if (translate_labels == True):\n",
    "                for key, value in label_dict.items():\n",
    "                    label = label.replace(key, value)\n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "            if (filter_labels == True):\n",
    "                if (label in keep_label):\n",
    "                    entities.append((start, end, label))\n",
    "                else:\n",
    "                    pass\n",
    "            else:\n",
    "                entities.append((start, end, label))\n",
    "        \n",
    "\n",
    "        training_data.append([(text, entities)])\n",
    "    return(training_data)\n",
    "\n",
    "\n",
    "def create_doc_bin(data: list, lang: str):\n",
    "    \"\"\"create a spacy DocBin file from training data/labeled data\n",
    "\n",
    "    Args:\n",
    "        data (list): list with training/annotated/labeled data\n",
    "        lang (str): spacy language, \"en\" for English, \"de\" for German\n",
    "    \"\"\"\n",
    "\n",
    "    nlp = spacy.blank(lang)\n",
    "\n",
    "    # the DocBin will store the documents\n",
    "    doc_bin = DocBin(attrs=[\"ENT_IOB\", \"ENT_TYPE\"])\n",
    "\n",
    "    for record in tqdm(data):\n",
    "        \n",
    "        # text are class list, need to be transformed to character\n",
    "        text = \" \".join(map(str,[el[0] for el in record]))\n",
    "        doc = nlp(text)\n",
    "\n",
    "        annotations = [item[1] for item in record]\n",
    "        # print(\"annotations:\")\n",
    "        # print(annotations)\n",
    "        ents = []\n",
    "        \n",
    "        for annotation in annotations[0]:\n",
    "            # add start, end and label as spans\n",
    "            start = annotation[0]\n",
    "            end = annotation[1]\n",
    "            label = annotation[2]\n",
    "            span = doc.char_span(start, end, label=label)\n",
    "            ents.append(span)\n",
    "        doc.ents = ents\n",
    "        doc_bin.add(doc)\n",
    "    return(doc_bin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing execution, split data in train and test (called `dev` here).\n",
    "\n",
    "The manually annotated data has too many labels for training: `\"PER\", \"LOC\", \"EVENT\", \"LAW\", \"ORG\", \"CLUB\", \"CITY\", \"COUNTRY\", \"MOVEMENT\", \"DATE\", \"ADR\", \"SLOGAN\", \"PUBLICATION\", \"PARTY\", \"AWARD\"`.\n",
    "So I "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep_label\n",
    "\n",
    "keep_label = [\"PER\", \"LOC\", \"EVENT\", \"LAW\", \"ORG\", \"CLUB\", \"CITY\", \"COUNTRY\", \"MOVEMENT\", \"PARTY\", \"AWARD\"]\n",
    "\n",
    "label_dict = {\"CLUB\":\"ORG\", \"PARTY\":\"ORG\", \"MOVEMENT\":\"EVENT\", \"AWARD\": \"EVENT\"}\n",
    "\n",
    "labeled_data = convert_rubrix_to_spacy(rubrix_name=\"chronik_annotations\", query=\"status:Validated\", filter_labels = True, translate_labels = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data and save in binary format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 186/186 [00:00<00:00, 1893.86it/s]\n",
      "100%|██████████| 47/47 [00:00<00:00, 1619.24it/s]\n"
     ]
    }
   ],
   "source": [
    "train, dev = train_test_split(labeled_data, test_size=0.2) \n",
    "create_doc_bin(train, \"de\").to_disk(\"data/train.spacy\")\n",
    "create_doc_bin(dev, \"de\").to_disk(\"data/dev.spacy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use a spacy config file\n",
    "- https://ner.pythonhumanities.com/03_02_train_spacy_ner_model.html#what-is-the-spacy-config-cfg-file-and-how-do-i-create-it\n",
    "- base config file from: https://github.com/wjbmattingly/holocaust_ner_lessons/blob/main/data/config.cfg\n",
    "- Spacy Documentation about projects https://explosion.ai/blog/spacy-v3-project-config-systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[38;5;1m✘ The provided output file already exists. To force overwriting the\n",
      "config file, set the --force or -F flag.\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create base config\n",
    "!python3 -m spacy init config --pipeline ner data/base-config.cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;3m⚠ Nothing to auto-fill: base config is already complete\u001b[0m\n",
      "\u001b[38;5;2m✔ Saved config\u001b[0m\n",
      "data/config.cfg\n",
      "You can now add your data and train your pipeline:\n",
      "python -m spacy train config.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\n"
     ]
    }
   ],
   "source": [
    "# fill with default values\n",
    "!python3 -m spacy init fill-config data/base-config.cfg data/config.cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate training data \n",
    "\n",
    "based on config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\n",
      "============================ Data file validation ============================\u001b[0m\n",
      "\u001b[38;5;2m✔ Pipeline can be initialized with data\u001b[0m\n",
      "\u001b[38;5;2m✔ Corpus is loadable\u001b[0m\n",
      "\u001b[1m\n",
      "=============================== Training stats ===============================\u001b[0m\n",
      "Language: de\n",
      "Training pipeline: tok2vec, ner\n",
      "186 training docs\n",
      "47 evaluation docs\n",
      "\u001b[38;5;2m✔ No overlap between training and evaluation data\u001b[0m\n",
      "\u001b[38;5;3m⚠ Low number of examples to train a new pipeline (186)\u001b[0m\n",
      "\u001b[1m\n",
      "============================== Vocab & Vectors ==============================\u001b[0m\n",
      "\u001b[38;5;4mℹ 8124 total word(s) in the data (2554 unique)\u001b[0m\n",
      "\u001b[38;5;4mℹ No word vectors present in the package\u001b[0m\n",
      "\u001b[1m\n",
      "========================== Named Entity Recognition ==========================\u001b[0m\n",
      "\u001b[38;5;4mℹ 7 label(s)\u001b[0m\n",
      "0 missing value(s) (tokens with '-' label)\n",
      "\u001b[38;5;3m⚠ Low number of examples for label 'LAW' (42)\u001b[0m\n",
      "\u001b[2K\u001b[38;5;3m⚠ Low number of examples for label 'CITY' (35)\u001b[0m\n",
      "\u001b[2K\u001b[38;5;3m⚠ Low number of examples for label 'COUNTRY' (25)\u001b[0m\n",
      "\u001b[2Kalyzing label distribution...\u001b[38;5;2m✔ Examples without occurrences available for all labels\u001b[0m\n",
      "\u001b[38;5;2m✔ No entities consisting of or starting/ending with whitespace\u001b[0m\n",
      "\u001b[38;5;2m✔ No entities crossing sentence boundaries\u001b[0m\n",
      "\u001b[1m\n",
      "================================== Summary ==================================\u001b[0m\n",
      "\u001b[38;5;2m✔ 6 checks passed\u001b[0m\n",
      "\u001b[38;5;3m⚠ 4 warnings\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# check data \n",
    "!python3 -m spacy debug data data/config.cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Saving to output directory: models/output\u001b[0m\n",
      "\u001b[38;5;4mℹ Using CPU\u001b[0m\n",
      "\u001b[1m\n",
      "=========================== Initializing pipeline ===========================\u001b[0m\n",
      "[2022-04-14 12:00:24,105] [INFO] Set up nlp object from config\n",
      "[2022-04-14 12:00:24,110] [INFO] Pipeline: ['tok2vec', 'ner']\n",
      "[2022-04-14 12:00:24,112] [INFO] Created vocabulary\n",
      "[2022-04-14 12:00:24,112] [INFO] Finished initializing nlp object\n",
      "[2022-04-14 12:00:24,487] [INFO] Initialized pipeline components: ['tok2vec', 'ner']\n",
      "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
      "\u001b[1m\n",
      "============================= Training pipeline =============================\u001b[0m\n",
      "\u001b[38;5;4mℹ Pipeline: ['tok2vec', 'ner']\u001b[0m\n",
      "\u001b[38;5;4mℹ Initial learn rate: 0.001\u001b[0m\n",
      "E    #       LOSS TOK2VEC  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
      "---  ------  ------------  --------  ------  ------  ------  ------\n",
      "  0       0          0.00     28.67    0.00    0.00    0.00    0.00\n",
      "  2     200        233.66   2644.35   26.83   41.25   19.88    0.27\n"
     ]
    }
   ],
   "source": [
    "!python3 -m spacy train data/config.cfg --output ./models/output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rosa Liste PARTY\n"
     ]
    }
   ],
   "source": [
    "trained_nlp = spacy.load(\"models/output/model-best\")\n",
    "text = 'Die Rosa Liste öffnet sich den Lesben: „Rosa Liste – jetzt lesbisch-schwul?“, eine Veranstaltung organisiert vom AK Uferlos. In der folgenden Stadtratswahl 1994 treten sowohl schwule als auch lesbische KandidatInnen an; Marion Hölczl war bereits ab 1992 Rosa-Liste-Bezirksrätin in Altstadt-Lehel.'\n",
    "doc = trained_nlp(text)\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print (ent.text, ent.label_)\n",
    "if len(doc.ents) == 0:\n",
    "    print (\"No entities found.\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e24addbd1c21af059ce21653e18ac38406b3fe69eeb60448b8d4120b64cb5797"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
