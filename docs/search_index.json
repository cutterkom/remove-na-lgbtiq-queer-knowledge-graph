[["index.html", "Remove NA About Creating a LGBTIQ* Knowledge Graph 1 Intro 1.1 Blog posts in English and German", " Remove NA About Creating a LGBTIQ* Knowledge Graph Katharina Brunner 1 Intro I will be allowed to spend a larger part of 2022 building a Knowledge Graph on queer history. This page will serve as the technical documentation. I also publish blog posts on topic related to this projects 1.1 Blog posts in English and German Remove NA: Warum ich einen LGBTIQ*-Knowledge Graph bauen werde RDF, RML, YARRRML: A basic tutorial to create Linked Data from a relational database table Stock Taking: How big is the queer Wikidata Vier Wochen Prototype Fund: Was bisher geschah "],["data-gathering.html", "2 Data Gathering 2.1 Infrastructure 2.2 Citavi data dumps 2.3 Entity Resolution 2.4 New database with cleaned entries 2.5 Named Entity Extraction", " 2 Data Gathering The first step is to merge different data sources. The scripts for this can be found in the repo under data-gathering. 2.1 Infrastructure 2.1.1 MariaDB Initial converted data from Citavi and other sources are stored in a MariaDB. Install and setup on Mac with homebrew. CREATE DATABASE lgbtiq_kg CHARACTER SET = &#39;utf8&#39; COLLATE = &#39;utf8_german2_ci&#39;; FLUSH PRIVILEGES; ALTER USER &#39;root&#39;@&#39;localhost&#39; IDENTIFIED BY &#39;NEW_PASSWORD&#39;; 2.2 Citavi data dumps Scripts: data-gathering/from-citavi/ There are three exports from Citavi data dumps that are normalized: poster, books, digital. All three are stored in a MariaDB. Naming convention: plurale: data -&gt; books_years singular: relations -&gt; book_years: relationship between book_id and year_id 2.2.1 Books data model books data model 2.2.1.1 Poster data model poster data model 2.2.2 Views 2.2.2.1 Books A wide view on all books CREATE VIEW books_wide AS SELECT books.book_id, books_authors.author_id, books_editors.editor_id, books_locations.location_id, books_publishers.publisher_id, books_series.series_id, books_years.year_id, books.name, books_authors.author, books.isbn, books.subtitle, books.title_supplement, books_editors.editor, books_locations.location, books_publishers.publisher, books_series.series, books_years.year FROM books -- add authors LEFT JOIN book_author ON books.book_id = book_author.book_id LEFT JOIN books_authors ON book_author.author_id = books_authors.author_id -- add editors LEFT JOIN book_editor ON books.book_id= book_editor.book_id LEFT JOIN books_editors ON book_editor.editor_id = books_editors.editor_id -- add locatiONs LEFT JOIN book_locatiON ON books.book_id= book_locatiON.book_id LEFT JOIN books_locatiONs ON book_locatiON.locatiON_id = books_locatiONs.locatiON_id -- add publishers LEFT JOIN book_publisher ON books.book_id= book_publisher.book_id LEFT JOIN books_publishers ON book_publisher.publisher_id = books_publishers.publisher_id -- add series LEFT JOIN book_series ON books.book_id= book_series.book_id LEFT JOIN books_series ON book_series.series_id = books_series.series_id -- add years LEFT JOIN book_year ON books.book_id= book_year.book_id LEFT JOIN books_years ON book_year.year_id = books_years.year_id; 2.2.2.2 Posters Wide View on posters data. CREATE VIEW posters_wide AS SELECT posters.poster_id, posters_authors.author_id, posters_years.year_id, posters_keywords.keyword_id, posters.title, posters.size, posters_authors.author, posters_years.year, posters_keywords.keyword FROM posters -- add authors LEFT JOIN poster_author ON posters.poster_id = poster_author.poster_id LEFT JOIN posters_authors ON posters_authors.author_id = poster_author.author_id -- add years LEFT JOIN poster_year ON posters.poster_id = poster_year.poster_id LEFT JOIN posters_years ON posters_years.year_id = poster_year.year_id -- add keywords LEFT JOIN poster_keyword ON posters.poster_id = poster_keyword.poster_id LEFT JOIN posters_keywords ON posters_keywords.keyword_id = poster_keyword.keyword_id ; 2.3 Entity Resolution Deduplication, entitiy resolultion, entity linking etc. is a main and re-accuring task in this project. A first setup to tackle the task is two-fold: Calculate text similarities between entitity stings based on cosine similarity Human decision making 2.3.1 Calculate text similarities between entitity stings based on cosine similarity based on quanteda scripts under data-gathering/entity-resolution cosine minimum similiarty is set to 0.75 (for authors and publishers from posters and books) 2.3.2 Human decision making Shiny app called “Who is Who?” to show possible matching candidates decision by button clicking code in apps/entity-resolver results saved in er_candidates 2.3.3 Adding new candidates To update the Shiny app in order to resolve new candidates these two steps are needed: get new additional_infos in get-additional-infos.R add new source to get_candidates_names() 2.4 New database with cleaned entries In lgbtiq_kg_clean live all deduplicated entities. So to say the end result of all scripts in data-gathering/entitiy-resolution View books_wide: CREATE VIEW books_wide AS SELECT entities.id, books_authors.book_id, entities.name, books.name AS title, books.subtitle, books.isbn FROM entities INNER JOIN books_authors ON entities.id = books_authors.id LEFT JOIN books ON books.book_id = books_authors.book_id; View psoters_wide: CREATE VIEW posters_wide AS SELECT DISTINCT entities.id, posters_authors.poster_id, entities.name, posters.title, posters.short_title, posters.filename, posters.size, posters_years.year FROM entities INNER JOIN posters_authors ON entities.id = posters_authors.id LEFT JOIN posters ON posters.poster_id = posters_authors.poster_id LEFT JOIN posters_years ON posters.poster_id = posters_years.year_id;; View entities_per_type: Which entity appears where? CREATE VIEW entities_per_type AS SELECT entities.id, entities.name, books_authors.book_id, posters_authors.poster_id FROM entities LEFT JOIN books_authors ON entities.id = books_authors.id LEFT JOIN posters_authors ON entities.id = posters_authors.id; 2.5 Named Entity Extraction One of the more challenging parts of the project is NER (Named Entity Extraction) based on NLP. 2.5.1 Source data My text data that I label comes from the Munich LGBTIQ* Chronicle: 233 entries of very dense infomation about the chronological order of events. See it yourself on forummuenchen.org. 2.5.2 Of-the-self NER models The first step was to use different off-the-shelf models and inspect the results in a rubrix webapp. Spacy Flair Bert-based model using huggingface transformers All three performed medicore, in three different ways. I decided to continue with spacy, because it has such an extensive documentation of training a NER model as well as using rule-based patterns. 2.5.3 Rule-based entity extraction Beside a model-based approach, I also use patterns to extract entities. In contrast to the former, this works quite well for date expressions and addresses. Script 2.5.3.1 Rules for extracting addresses 2.5.3.2 Rules for extracting date related infos 2.5.3.3 Extract entities from lists Also, I look up entities, I have in the database already. This seems not to work off-the-self very well. For example, when Landeshauptstadt München is registered as a publisher or author, it overwrites the location entity for “München”. Really not what I want. 2.5.4 Model-based approach: Labeling and training Code in Notebook I labeled the data using rubrix and used quite a lot of labels: 15. There is a conflict of goals: for the structured, linked data, I want the annotations to be as detailed as possible. However, for training a NER model I need larger amounts of data and it is smarter to use labels that unify several domains under themselves. For example: Clubs and organisations both run under ORG. The distribution is quite skewed. There are labels like AWARD or Slogan that are definitly to specific to use in a training. Therefore, also the trained model does not perform very well, diplomatically speaking. See: That’s why I decided to combine labels, e.g. all CLUBs will be a ORG and use only a subpart of labels: keep_label = [&quot;PER&quot;, &quot;LOC&quot;, &quot;EVENT&quot;, &quot;LAW&quot;, &quot;ORG&quot;, &quot;CLUB&quot;, &quot;CITY&quot;, &quot;COUNTRY&quot;, &quot;MOVEMENT&quot;, &quot;PARTY&quot;, &quot;AWARD&quot;] label_dict = {&quot;CLUB&quot;:&quot;ORG&quot;, &quot;PARTY&quot;:&quot;EVENT&quot;, &quot;MOVEMENT&quot;:&quot;EVENT&quot;, &quot;AWARD&quot;: &quot;EVENT&quot;} This improved the best models metrics a bit, but still too bad for usage: &quot;performance&quot;:{ &quot;ents_f&quot;:0.5130111524, &quot;ents_p&quot;:0.6699029126, &quot;ents_r&quot;:0.4156626506, &quot;ents_per_type&quot;:{ &quot;LAW&quot;:{ &quot;p&quot;:0.7, &quot;r&quot;:0.7, &quot;f&quot;:0.7 }, &quot;ORG&quot;:{ &quot;p&quot;:0.5714285714, &quot;r&quot;:0.4666666667, &quot;f&quot;:0.5137614679 }, &quot;PER&quot;:{ &quot;p&quot;:0.9, &quot;r&quot;:0.5142857143, &quot;f&quot;:0.6545454545 }, &quot;EVENT&quot;:{ &quot;p&quot;:0.5, &quot;r&quot;:0.2222222222, &quot;f&quot;:0.3076923077 }, &quot;LOC&quot;:{ &quot;p&quot;:0.7272727273, &quot;r&quot;:0.2424242424, &quot;f&quot;:0.3636363636 }, &quot;CITY&quot;:{ &quot;p&quot;:0.75, &quot;r&quot;:0.4285714286, &quot;f&quot;:0.5454545455 }, &quot;COUNTRY&quot;:{ &quot;p&quot;:1.0, &quot;r&quot;:0.3333333333, &quot;f&quot;:0.5 } } "],["data-modelling.html", "3 Data Modelling 3.1 Create RDF files from database", " 3 Data Modelling In this document you’ll find explainations and descriptions of the data model used in the Remove NA project. Remove NA collects data about real world entities, such as people, organizations or locations, and the relationships between them. 3.1 Create RDF files from database RDF files created with RML rules. Scripts in data-modelling run parser: Knowledge Graph v1 with de-duplicated books, authors, publishers (best version!): yarrrml-parser -i mappings/kg_v1.yml -o mappings/kg_v1.r2rml.ttl -f R2RML for posters: yarrrml-parser -i mappings/posters-rules.yml -o mappings/posters-mapping.r2rml.ttl -f R2RML for books: yarrrml-parser -i mappings/books-rules.yml -o mappings/books-mapping.r2rml.ttl -f R2RML for books and authors yarrrml-parser -i mappings/books-authors.yml -o mappings/books-authors.r2rml.ttl -f R2RML create RDF files with python3 create-rdf.py "],["data-linking.html", "4 Data Linking 4.1 GND/lobid 4.2 VIAF", " 4 Data Linking The internal data are linked to various external data sources. Main database table el_matches. Idea is to just dump everything there in a first step. additional_data is a json column for maximal flexibility CREATE TABLE `el_matches` ( `created_at` timestamp NULL DEFAULT current_timestamp(), `updated_at` timestamp NULL DEFAULT NULL ON UPDATE current_timestamp(), `entity_id` varchar(55) NOT NULL, `entity_id_type` varchar(255) DEFAULT NULL, `entity_id_combination` varchar(255) DEFAULT NULL, `entity_id_combination_type` varchar(255) DEFAULT NULL, `external_id` varchar(255) DEFAULT NULL, `external_id_label` varchar(255) DEFAULT NULL, `external_id_type` varchar(55) NOT NULL, `external_id_desc` varchar(255) DEFAULT NULL, `source` varchar(255) DEFAULT NULL, `source_id` longtext DEFAULT NULL, `property_type` varchar(255) DEFAULT NULL, `additional_data` longtext CHARACTER SET utf8mb4 COLLATE utf8mb4_bin DEFAULT NULL CHECK (json_valid(`additional_data`)) ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4; 4.1 GND/lobid I want to link the entities, like a book author or publisher, to the external identifier of the German National Library (GND). The easiest way seems to be calling the lobid API. It returns a json with meta data, e.g. contributors or topics that often, but not always, have a GND id. I filter the json file by using jq, a json processor, that has a wrapper in R: jqr. 4.1.1 Output DB tbl: lgbtiq_kg.el_matches beware: it’s not cleaned data, so not in “clean” database lgbtiq_kg_clean 4.1.2 Examples for jq commands getting contributors: .member[].contribution[]? | {type: .agent?.type[], label: .agent?.label, role:.role?.id, gnd_id: .agent?.gndIdentifier} getting topics: .member[].subject[].componentList[]? | {gnd_id: select(.type?).gndIdentifier, type: select(.gndIdentifier?).type[], label: select(.type?).label} getting lobid ressource id: .member[] | {source_id: .id} more complex with optional fields: .member[] | {id: ((select(.sameAs) | {sameAs}) // null), gnd_subject_category: .gndSubjectCategory, placeOfBusiness: .placeOfBusiness} 4.1.3 Basic workflow Most sophisticated script: data-linking/lobid/04-search-via-publisher.R serves for templates from now on search for isbn the most precise way, because it’s already a persistent id data-linking/lobid/01-search-via-isbn.R script first version search for author name and title in the relevant objects query like contribution.agent.label:AUTHOR+AND+title: TITLE&amp;format=json\") data-linking/lobid/02-search-via-author-title.R search just by name data-linking/lobid/03-search-via-author.R search for publishers as corporate bodies: data-linking/lobid/04-search-via-publisher.R 4.2 VIAF To VIAF by using the viafr R-package. It suggest a VIAF entity when providing a string. db table: el_viaf_books_authors "],["misc.html", "5 Misc 5.1 Virtual Environments and dependency management 5.2 Render docs", " 5 Misc 5.1 Virtual Environments and dependency management Python venv R: renv.lock (one has to activate it with renv::activate()), see more in docs 5.2 Render docs Render docs: bookdown::render_book(\"docs/\", output_dir = \".\") "]]
